Model name,Main changes,RMS train CV mean,RMS train CV std,RMS test,Notes,z-score of test,filename,
Dummy predictor,,0.119,0.012,0.126,Test is a bit worse than train but not too shabby,0.58,,
Linear regression,,0.110,0.011,0.112,,0.19,,
Random forest untuned,,0.112,0.008,,,,,
Random forest tuned,,0.108,0.010,,,,,
Random forest tuned,Changing to scipy distributions when possible and increasing number of iterations from 20 to 40,0.106,0.008,,,,,
Random forest tuned,Changing back to 10 iter,0.109,0.009,,,,,
Random forest tuned,"Bug, ran again",0.107,0.008,0.113,The linear model is 34.6% change from predict majority but this model is only 19.7% different. Slightly worse on testing data than the linear model.,0.73,,
Linear regression,added time-lagged features,0.119,0.014,,,,,
RF,,0.112,0.008,,,,,
Linear regression,trying RobustScaler,0.110,0.011,,,,,
RF,,0.112,0.008,,,,,
LR,Upsampling and interpolating long-term to get better resolution,0.110,0.011,,,,,
RF,,0.112,0.008,,,,,
LR,Adding umbra and penumbra,0.110,0.011,,The umbra and penumbra features don't seem to be helpful whatsoever,,,
RF,,0.112,0.008,,,,,
LR,Adding dmop hourly,123409588.906,134174838.532,,Something bad happened!,,,
RF,,0.112,0.009,,Didn't do anything for RF either,,,
LR,,,,,,,,
Bagged LR,"0.9 samples, 0.8 features",0.107,0.009,,Okay improvement but within a sigma,,,
RF,Tuned 10 iter,0.108,0.009,,Slightly worse than the bagged LR but WAAAAY slower,,,
GB,untuned,0.107,0.009,,,,,
Adding back events and DMOP to see if some methods take better advantage of them,,,,,,,,
LR,32990383.388,28456407.480,,,I think some of the fields don't exist in some of the training sets so probably have random weights,,,
Bagged LR,,,,,Wouldn't complete,,,
RF tuned,,,,,,,,
GB,,,,,,,,
Removing DMOP,,,,,,,,
LR,,0.110,0.011,,,,,
Bagged LR,,0.108,0.009,,Slightly worse but it's random,,,
RF,tuned,0.106,0.008,,,,,
GB,,0.105,0.008,,Seems like RF and GB are better able to handle umbra and penumbra events,,,
Making umbra and penumbra mutually exclusive,,,,,,,,
LR,,0.110,0.011,,,,,
Bagged LR,,0.109,0.009,,,,,
RF,tuned,0.108,0.009,,,,,
GB,,0.105,0.008,,,,,
Removing pho/dei umbra/pen,,,,,,,,
LR,,0.110,0.011,,,,,
Bagged LR,,0.108,0.009,,,,,
RF,tuned,0.107,0.009,,,,,
GB,,0.106,0.008,,,,,
Adding umbra and penumbra durations,,,,,,,,
LR,,0.110,0.011,,,,,
Bagged LR,,0.108,0.009,,,,,
RF,tuned,0.106,0.008,,,,,
GB,,0.105,0.008,,,,,
Adding durations to the second slice of the event,,,,,,,,
Dummy predictor,,0.115,0.017,,Uhhh this shouldn't change but it did,,,
LR,,0.181,0.093,,,,,
Bagged LR,,0.154,0.057,,,,,
RF,Tuned,0.107,0.014,,,,,
GB,,0.107,0.008,,,,,
"Removing durations, adding SVR",,,,,,,,
LR,,0.110,0.011,,,,,
Bagged LR,,0.107,0.008,,,,,
LinearSVR,,0.112,0.008,,It's worse and it took ages to train,,,
RF,tuned,0.109,0.008,,,,,
GB,,0.106,0.008,,,,,
Adding MRB and MSL range indicators,,,,,,,,
LR,,0.109,0.011,,,,,
Bagged LR,,0.108,0.010,,,,,
LinearSVR,,0.118,0.016,,,,,
RF,tuned,0.107,0.009,,,,,
GB,,0.106,0.008,,,,,
Adding lagged versions,,,,,,,,
LR,,12.599,17.664,,,,,
Bagged LR,,5.337,7.396,,Note that RF wasn't impacted at all,,,
GB,,0.108,0.011,,Taking a while,,,
"Removing the lagged versions of angles, switching to 14d for sunmarskm and eclipsedurationmin",,,,,,,,
LR,,0.111,0.012,,This is worse in both mean and std,,,
Bagged LR,,0.106,0.009,,This might be the best model ever but it seems like we probably got lucky,,,
GB,,0.109,0.013,,This is worse in both...,,,
Trying without lagged version of sunmarskm but including basic eclipse and lagged version,,,,,,,,
LR,,0.110,0.011,,Basically back to normal,,,
Bagged LR,,0.108,0.010,,,,,
GB,,0.110,0.013,,This is the worse ever for gradient boosting,,,
"Removing eclipse lagged, adding higher resolution saaf data",,,,,,,,
LR,,0.110,0.011,,,,,
Bagged LR,,0.107,0.009,,,,,
GB,,0.111,0.014,,Worse ever,,,
Added 1hour count of dmop commands,,,,,,,,
LR,,0.108,0.011,,Much better!,,,
Bagged LR,,0.106,0.009,,Also better,,,
GB,,0.108,0.012,,"In feature importances, dmop counts were fourths after days in space and sz and sy",,,
Adding 7 day dmop count average,,,,,,,,
LR,,0.109,0.010,,About the same,,,
Bagged LR,,0.107,0.009,,About the same,,,
GB,,0.108,0.011,,About the same,,,
Adding event data 1H counts,,,,,,,,
LR,,0.108,0.010,,,,,
Bagged LR,,0.105,0.008,0.107,Best ever!,0.73,,
GB,,0.110,0.015,,,,,
Setting many features to qcut 20,,,,,,,,
LR,,0.108,0.010,,,,,
Bagged LR,,0.107,0.008,,"Worse, sad days",,,
GB,,0.109,0.013,,,,,
"Removing qcuts, adding sin and cos of the angles",,,,,,,,
LR,,0.108,0.010,,,,,
Bagged LR,,0.105,0.008,,,,,
GB,,0.111,0.015,,Note has the feature importances on the top outputs,,,
Removing days_in_space,,,,,,,,
LR,,0.107,0.009,,better,,,
Bagged LR,,0.106,0.008,,similar,,,
GB,,0.116,0.016,,WAAAAY worse,,,
"Adding days in space back, working on hyperparameter tuning setup",,,,,,,,
LR,,0.108,0.010,,,,,
Bagged LR,,0.106,0.009,,,,,
RF,tuned,0.104,0.009,,"Whoa, best ever",,,
RF,tuned,0.105,0.010,,Also quite good!,,,
"More aggressive hyperparameter tuning - adjusted some of the tuning ranges, added tuning for GB",,,,,,,,
RF,tuned,0.103,0.008,,Best ever,,,
Bagged LR,tuned,0.103,0.007,,Trying this out for the first time but it doesn't seem to work unless I set n_jobs=1 so it's slow. But these are the best results ever so I shouldn't complain and I can save the hyperparams,,,
Bagged LR,tuned,0.105,0.006,,Bit of variation here unfortunately,,,
RF tuned,"26 features, RandomForestRegressor(80, min_samples_leaf=30, max_depth=15, max_features=15)",0.103,0.009,0.116,,1.43,,
Bagged LR,tuned frozen,0.105,0.008,0.105,,-0.01,,
Bagged LR,Trying with 30 iter,0.105,0.008,,,,,
BLR,frozen,0.104,0.008,,,,,
BLR,"retuned, 30 estimators, 20 iterations",0.104,0.008,,It's pretty amazing that there's no difference; these models are individually tuning the hyperparams.,,,
GB,untuned,0.112,0.017,,Man this is awful.,,,
GB,tuned 20 search iter,0.102,0.009,,"Best ever, though probably it won't generalize",,,
AdaBoost LR,With params tuned to 7D,0.110,0.008,,,,,
AdaBoost LR,Just hung... probably the n_jobs bug got me again,,,,,,,
Redoing tests in prep for updated data,,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozen params,0.104,0.007,,,,,
RF,frozen params,0.103,0.009,,,,,
After updating data,,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozen params,0.104,0.007,0.108,Unfortunately it's worse,0.48,,
RF,frozen params,0.103,0.009,,,,,
Adding sin/cos of sunmarsearth angle,,,,,,,,
LR,,0.108,0.010,,,,,
BLR,,0.105,0.008,,,,,
RF,,0.104,0.009,,It's pretty much worse all around,,,
"Removing sin/cos of sunmarsearth, removing mars penumbra, removing msl range",,,,,,,,
LR,,0.108,0.010,,,,,
BLR,,0.104,0.008,,,,,
RF,,0.103,0.009,,"It seems very slightly worse, could be that penumbra was useful?",,,
"Dropping sx, sa features",,,,,,,,
LR,,0.108,0.010,,Interesting.. not really any change,,,
BLR,,0.104,0.008,,Again no real change,,,
RF,,0.106,0.009,,huh big change here,,,
Dropping mars umbra and mrb range,,,,,,,,
LR,,0.108,0.010,,About the same,,,
BLR,,0.105,0.008,,"worse, probably",,,
RF,,0.107,0.010,,,,,
"Trying 2, 5, 10 day rolling mean of eclipse and sunmars dist",,,,,,,,
LR,,40.204,56.710,,Triggers the overfit,,,
BLR,,2.744,3.735,,same,,,
RF,,0.105,0.008,,Gets an improvment here so there must be some way,,,
"Dropping the sunmars one, trying int means of the eclipse features",,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozen,0.104,0.008,,Bit better,,,
RF,frozone,0.105,0.009,,,,,
Trying with just fillna no int conversion,,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozone,0.105,0.007,,Worse,,,
RF,frozone,0.105,0.009,,Worse,,,
"Add int conversion back, change the global fillna to backfill",,,,,,,,
LR,,0.108,0.009,,Same,,,
BLR,frozone,0.105,0.006,,Seems bad somehow but the std is at least lower,,,
RF,frozone,0.107,0.008,,,,,
"Changing dmop mean lag feature to do integer mean, revert global fillna changes",,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozone,0.104,0.008,,Looks decent,,,
RF,frozone,0.105,0.009,,whoa,,,
"Adding dmop lags for 2, 10 hours",,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozone,0.103,0.007,,Big improvement!!! Probably it's because of smoothing,,,
RF,frozone,0.104,0.009,,,,,
Adding dmop 40h lag,,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozone,0.105,0.008,,Whoa way worse... maybe the previous .103 was just luck,,,
RF,frozone,0.104,0.008,,About the same,,,
Redo previous test,,,,,,,,
LR,,0.108,0.010,,,,,
BLR,frozone,0.104,0.007,,Ok I probably shouldn't have drawn conclusions then,,,
RF,frozone,0.104,0.009,,,,,
Lars,First try................,27076.605,35751.418,,,,,
"Dmop dropping 7 days, changing 10 hours to 5 also",,,,,,,,
LR,,0.107,0.011,,Better but within stddev,,,
BLR,,0.104,0.009,,,,,
RF,,0.105,0.009,,,,,
Lars,,1555.174,1099.739,,,,,
Adding event counts 2 and 5 hour lags,,,,,,,,
LR,,0.107,0.011,,,,,
BLR,,0.103,0.008,,Progress maybe,,,
RF,,0.103,0.009,,,,,
"Changing eclipseduration stuff to use the shared code, dropping 10 day",,,,,Expect no difference,,,
LR,,0.107,0.011,,,,,
BLR,,0.103,0.008,,,,,
RF,,0.105,0.009,,huh big change,,,
Interpolate all fields before global fillna,,,,,Some of the fields like angles are missing values for no apparent reason,,,
LR,,0.107,0.010,,Slight improvement,,,
BLR,,0.103,0.007,,Feeling good,,,
RF,,0.104,0.008,,,,,
sin and cos at 2 decimals,,,,,,,,
LR,,0.107,0.010,,,,,
BLR,,0.104,0.008,,Huh it seems to hurt a lot actually,,,
RF,,0.104,0.009,,,,,
Lars,,2.040,2.565,,,,,
Adding them back (plain test),,,,,,,,
LR,,0.107,0.010,,,,,
BLR,,0.103,0.007,,,,,
Removing the non-sincos versions,,,,,,,,
LR,,0.107,0.010,,,,,
BLR,,0.104,0.008,,bit worse,,,
RF,,0.105,0.008,,,,,
Lars,,0.372,0.324,,huh,,,
AdaLR,,0.109,0.007,,,,,
AdaLR,retuned,0.117,0.009,,,,,
NN tests,,,,,,,,
NN,Lightly tuned to 7days,0.105,0.008,,Did a very quick tuning on the 7d version but was at the edge of the range for dropout,,,
NN,Tuned more,,,,Canceled the fine-grained tuning cause it kept crashing but got the summary results,,,
NN,,,,,"Tried tuning more heavily on 7D, not really strong conclusions",,,
Syncing with test,,,,,,,,
BLR,,0.103,0.008,0.118,Whoa really really bad correlation,2.01,,
NN,Some tuning,0.105,0.008,0.102,,,,
NN input noise,,,,,,,,
NN,Tuning input noise to 7D,,,,"Some preference for input noise, generally over 5% noise",,,
NN,Tuning input noise to 1H,0.104,0.007,,This is overtuned a bit but generally input noise seems weakly helpful,,,
LR,Test 7D,0.068,0.014,,,,,
GaussianProcess,Test 7D,0.072,0.010,,,,,
GaussianProcess,Test 7D. Trying the params from sklearn example,0.072,0.010,,Doesn't make any difference,,,
NN,Test 7D,0.062,0.009,,Hyperparam autocorrelation seems to work. Learning rate is more important than random noise in this test.,,,
BLR,Test 7D,0.059,0.007,,,,,
Rebaseline before dropping data with missing power info,,,,,,,,
LR,,0.107,0.010,,,,,
BLR,,0.103,0.007,,,,,
NN,,0.105,0.007,,,,,
RF,,0.105,0.008,,,,,
GP,,,,,Exploded my memory to like 37gb before I killed it. Fuck GP.,,,
Dropping about 300 rows of training data with missing power values,,,,,,,,
LR,,0.107,0.011,,,,,
BLR,,0.103,0.008,,Fairly minor diff but it's a little improvement,,,
NN,,0.105,0.008,,,,,
RF,,0.104,0.009,,,,,
Adding FTL flagcomms indicator,,,,,,,,
LR,,0.106,0.010,,Best ever for plain LR,,,
BLR,,0.103,0.008,,About the same,,,
NN,,0.103,0.007,,Better,,,
RF,,0.103,0.008,,Also better,,,
Flagcomms 1 and 2 hour means,,,,,,,,
LR,,0.105,0.010,,Best ever for plain LR,,,
BLR,,0.102,0.007,,Best ever for BLR,,,
NN,,,,,,,,
RF,,0.102,0.009,,Also quite good,,,
Adding all other FTL events over count 100,,,,,,,,
LR,,0.105,0.011,,Very slightly better,,,
BLR,,0.104,0.008,,I'm guessing that I need to tweak the max features again,,,
NN,,,,,,,,
RF,,0.099,0.010,,Whoa....,,,
Adding back the 3 evtf ranges with the new logic and 1h lag variants,,,,,,,,
LR,,0.104,0.011,,Best yet for LR,,,
BLR,,0.104,0.008,,I think the FTL events really screwed the BLR model,,,
NN,,,,,,,,
RF,,0.099,0.009,,About the same really,,,
Adjusting hyperparams based on 7D tuning,,,,,,,,
LR,,0.106,0.012,,This might be the result of that multiplied feature actually,,,
BLR,18 feat,0.102,0.008,,Best yet for BLR,,,
NN,,,,,,,,
RF,,0.101,0.010,,,,,
BLR,30 feat (50%),0.102,0.008,,"About the same really, going to try submitting this one",,,
RF,Same params,0.101,0.010,,,,,
Baseline with tuning,,,,,,,,
LR,,0.106,0.012,,,,,
BLR,,0.101,0.008,,Hyperparam tuning didn't help nurmerically,,,
RF,,0.101,0.010,,,,,
RF,Tuned,0.1005,0.01,,The retuned one,,,
NN,,0.102,0.009,,,,,
NN,Tuned,0.101,0.010,,Tuning results are fascinating,,,
Dropping the combo feature and retraining without tuning,,,,,,,,
LR,,0.104,0.011,,,,,
BLR,,0.100,0.007,,Best evar,,,
RF,,0.099,0.009,,,,,
NN,,0.140,0.060,,Having some troubles,,,
NN,,0.102,0.009,,"It seems like StandardScaler really fucks it up, maybe outliers?",,,
BLR,,0.099,0.007,0.104,Better for BLR but doesn't match the NN submission,0.69,,
NN,Added a tiny L2,0.101,0.007,,Can't seem to get it to actually produce output cause of the NaN bug,,,
Dropping sy features entirely,,,,,,,,
LR,,0.104,0.010,,,,,
BLR,,0.100,0.007,,,,,
RF,,0.100,0.010,0.110,20.71% diff from reg,0.99,,
NN,,0.100,0.007,0.100,54% diff from majority huh,0.05,,
Trying PCA,,,,,,,,
NN,Tuned with PCA,0.100,0.010,,Only significant param is sigmoid > relu = elu,,,
NN,Tuned without PCA,0.100,0.008,,"minibatch > full, less L2 is better, higher learning rate, higher input noise, possibly ELU but not sign",,,
"Working on NN after all the issues, now includes sa, sy, and altitude but removes sin/cos features",,,,,,,,
NN,,0.116,,,,,,
NN,No early stopping,0.109,,,,,,
NN,"no ES, 2x iter, half noise",0.116,,,,,,
NN,"revert previous, feature selection",0.114,,,,,,
NN,no feature selection,,,,,,,
LR,,0.104,0.010,,About the same as always,,,
BLR,,0.100,0.008,,About the same as always,,,
RF,,0.102,0.009,,Worse than before but probably closer to test,,,
GB,,0.100,0.010,,I'm cautiously optimistic,,,
NN,,0.108,0.013,,Okay fine I'll skip feature selection then,,,
Sync with test,,,,,,,,
RF,,0.101,0.010,0.110,20.08% avg deviation. About the same test as before.,0.90,,
GB,,0.103,0.010,0.117,70.87% avg deviation...,1.56,,
NN,Reduced the learning rate back to the old one,0.103,0.013,0.102,102.18% average deviation what,-0.07,Seems like the diff is dropping sy,
Derived features,,,,,,,,
LR,,0.105,0.010,,,,,
BLR,,0.099,0.008,,Very minor improvement,,,
RF,,0.101,0.009,,,,,
NN,With early stopping,0.101,0.010,,,,,
NN,0.05*mean FS,0.102,0.012,,,,,
BLR,Retuning,0.100,0.007,,Slightly worse but lower stddev,,,
"Dropping DMOP_event_counts_rolling_2h cause of gradient, occultationduration_min cause of log",,,,,,,,
LR,,0.104,0.010,,,,,
BLR,,0.099,0.007,,,,,
RF,,0.101,0.010,,,,,
NN,With PCA,0.107,0.013,,,,,
NN,"With PCA, retuned",0.098,0.009,,"Hyperparam tests very interesting, also ran pretty fast cause of early stopping",,,
BLR,With PCA,0.103,0.009,,,,,
Trying PCA for all,,,,,,,,
LR,,0.122,,,,,,
BLR,,0.120,,,I have a feeling I need to scale it before running PCA,,,
RF,,0.131,,,,,,
PCA for all with RobustScaler first,,,,,,,,
LR,,0.105,,,,,,
BLR,,0.103,,,,,,
RF,,0.105,,,Finally I find something in which RF is bad,,,
NN,,0.101,,,,,,
PCA for all with ExtraRobustScaler,Diffs from RobustScaler,,,,Thoughts: Doesn't really matter now,,,
LR,0.003,0.108,,,Way worse,,,
BLR,-0.001,0.102,,,Better,,,
RF,0.000,0.105,,,Same,,,
NN,0.000,0.101,,,Same,,,
Experiments in output scaling - transforms outputs for the internal model and untransforms when predicting,,,,,,,,
BLR,StandardScaler,0.102,0.009,,"Not much different, phew",,,
BLR,non,0.103,0.009,,,,,
BLR,StandardScaler,0.103,0.009,,,,,
BLR,PCA 0.995 whitened,0.103,0.009,,PCA and Standard not really different,,,
BLR,PCA 0.995 not whitened,0.105,0.009,,,,,
BLR,StandardScaler,0.102,0.009,,,,,
BLR,PCA 0.995 whitened,0.103,0.010,,,,,
BLR,StandardScaler + PCA,0.103,0.010,,Basically none of the output transforms matter at all for BLR,,,
RF,StandardScaler,0.108,0.009,,,,,
RF,PCA,0.107,0.009,,,,,
RF,StandardScaler + PCA,0.108,0.009,,All the RF tests are worse,,,
RF,PCA .999,0.107,0.009,,No different,,,
"Back to normal, still with PCA transform",,,,,,,,
LR,,0.108,0.014,,,,,
BLR,,0.102,0.009,,,,,
RF,,0.105,0.009,,,,,
GB,,0.099,0.007,,Darn... I was hoping that the PCA transform would ruin GB also,,,
NN,,0.101,0.011,,,,,
NN,Removed accidental double PCA,0.099,0.009,,,,,
PCA @ 0.999 instead of 0.99,,,,,,,,
LR,,1.319,1.722,,Whoa,,,
BLR,,0.104,0.011,,"Little bit worse, prob similar to LR baseline",,,
RF,,0.105,0.009,,Same,,,
NN,,0.316,0.012,,Diverged!,,,
Whoa back to 0.99,,,,,,,,
LR,,0.108,0.014,,,,,
BLR,,0.102,0.009,,,,,
RF,,0.105,0.009,,,,,
NN,,0.099,0.010,,,,,
Trying a bunch of time-lagged features based on automated search. Some are very long range like 1600hours = 64 days,,,,,,,,
LR,,0.110,0.015,,,,,
BLR,,0.102,0.009,,,,,
RF,,0.105,0.008,,,,,
NN,,0.100,0.010,,,,,
Trying again without PCA,,,,,,,,
LR,,0.103,0.009,,"Pretty good, have achieved similar before though",,,
BLR,,0.101,0.008,,Not too different than it was before,,,
RF,,0.106,0.004,,Huh,,,
NN,Not tuned for non-PCA,,,,,,,
PCA .992,I tried .999 but LR and NN diverge,,,,,,,
LR,,0.109,0.015,,,,,
BLR,,0.101,0.008,,,,,
RF,,0.104,0.008,,,,,
NN,,0.099,0.010,,,,,
Dropping the new features,,,,,,,,
LR,,0.146,,,,,,
BLR,,0.102,,,,,,
RF,,0.105,,,,,,
NN,,0.102,,,Basically the PCA variance threshold is dependent on the number of features.,,,
Revert,,,,,,,,
NN,tuned,0.098,,,MAE typically bad with some good ones. Higher input noise generally good.,,,
NN,Slight tweak,0.099,0.009,0.103,114.18% test deviation. I wonder if the PCA is breaking the generalization?,0.47,,
BLR,Without PCA,0.099,0.002,0.103,33.17% deviation huh,1.86,,
Redo baseline,,,,,,,,
LR,,0.103,0.009,,,,,
BLR,,0.100,0.008,,,,,
RF,,0.106,0.004,,,,,
NN,No PCA,0.098,0.008,,,,,
Adding some DMOP triggers,,,,,,,,
LR,,0.106,0.106,,,,,
BLR,,0.102,0.008,,,,,
RF,,0.106,0.005,,,,,
NN,,0.316,0.012,,All experiments are worse!,,,
Reducing DMOP windows to 2 hours,,,,,,,,
LR,,0.102,0.009,,,,,
BLR,,0.099,0.008,,Quite good BLR results,,,
RF,,0.105,0.006,,,,,
NN,,0.316,0.012,,Still diverges,,,
Trying to change event series to add 1 so that it's not exactly an indicator anymore,,,,,,,,
LR,,0.102,0.009,,,,,
BLR,,0.100,0.009,,,,,
RF,,0.105,0.006,,,,,
NN,,0.316,0.012,,,,,
Reverting and replacing the indicator cols with 1h rolling mean and changing DMOP forward interval to 1 hour,,,,,,,,
LR,,0.102,0.009,,,,,
BLR,,0.099,0.008,,,,,
RF,,0.105,0.005,,,,,
NN,,0.316,0.012,,,,,
Adding PCA 0.993,,,,,,,,
LR,,0.119,0.028,,,,,
BLR,,0.101,0.009,,,,,
RF,,0.105,0.008,,,,,
NN,,0.098,0.010,0.106,"Huh, pretty bad actually",0.78,,
"Trying RNN again with 1D samples, this time lower learning rate, input noise, some dropout, early stopping, and a little code cleanup",,,,,,,,
LR,,0.088,0.025,,,,,
BLR,,0.067,0.007,,,,,
RF,,0.073,0.008,,,,,
NN,,0.070,0.006,,,,,
RNN,,nan,,,"Split 1 trained fine, other two NaN",,,
RNN,maxnorm,nan,,,Same as previous test,,,
RNN,"upping rec dropout from 0.2 to 0.5, reg dropout from 0.4 to 0.5, reducing frmo 5 to 3 time steps",,,,Immediate NaN on all 3!,,,
RNN,no dropout at all,nan,,,,,,
RNN,LR 1e-4,nan,,,,,,
RNN,LR 1e-5,nan,,,I double checked and the unit test still works just fine,,,
RNN,Upping from 3 to 10 time samples,nan,,,,,,
RNN,"Back to 5 time, 0.2 rec drop, 0.4 reg drop but still have the lower learning rate",,,,First split converges... sloooowly. Second immediate NaN. Third immediate NaN.,,,
RNN,"Back to 1e-3 LR, trying full batch",,,,First split first epoch ok but then diverges,,,
RNN,Batch 64,0.096,,,"First split trains quite nicely, much lower error than before. Second split is also very nice. Third also completes, phew. Compared to the regular NN this gets better train error but worse test error. Also, compare to previous RNN result at same batch size but no noise etc 
0.0795 +/- 0.0128",,,
NN,Batch 64,0.067,0.007,,Settles to lower loss in all splits. Also doesn't trigger early stopping,,,
NN,"B64, early stop 10% val loss",0.072,0.010,,,,,
RNN,"B64, early stop 10% val loss",0.097,0.020,,Some of them early stopped a little bit,,,
NN,Don't think I changed anything,0.069,0.006,,,,,
RNN,Dropout matches NN dropout now,0.071,0.008,,Pretty close actually!,,,
NN,No change,0.070,0.007,,,,,
RNN,Reducing from 100 to 50 LSTM. Still double the NN weights but interesting to test,0.070,0.010,,About the same but higher stddev,,,
NN,No change,0.068,0.006,,,,,
RNN,Baseline,0.070,0.008,,Okay it's at least reasonable,,,
RNN,Tuning woooo,0.065,0.007,,The big takeaway from this is that similar perf to NN is possible. The range of fits was 0.065 to 0.31 so it's pretty sensitive. The main factor is learning rate - the lower learning rates really really kill the RNN. I didn't have the assert-finite bypass in place yet so they all converged. Higher LR is better and more recurr dropout is better. Validation early stopping didn't matter.,,,
Back to hourly samples,,,,,,,Values before all the RNN work,
LR,,0.135,0.048,,This LR result is worse than baseline... something is horribly wrong,,0.119,-0.016
BLR,,0.102,0.009,,,,0.101,-0.002
RF,,0.104,0.008,,,,0.105,0.001
NN,"Batch 64 still, really just testing the learning graph",0.102,0.011,,Batch size and val are the changes,,0.098,-0.004
RNN,Same settings tuned for 1D,0.098,0.010,,"This might be the best result ever, but by a small margin.",,,
NN,Retuning,0.102,0.014,,I must've broken something with the NN.....,,,
RNN,,0.096,0.008,,"20483 params vs 15,433 for the NN. Also X is small - 1mb so time inflation only makes it 5mb.",,,
RNN,Tuned,,,,"Several params worked fine but one caused NaN in the internal validation, trying again without validation",,,
NN,early stop on training,0.102,0.010,,I'm not sure what I changed but something about the NN is getting consistently worse results. I think it has something to do with features cause the LR baseline is consistently worse also.,,,
RNN,Untuned,0.097,0.009,,,,,
RNN,Tuned,0.095,0.009,,"Recurrent dropout important, learning rate important, 3 time steps generally beats 5",,,
"Rebaseline without PCA, first RNN without PCA",,,,,,,,
LR,,0.103,0.009,,,,,
BLR,,0.101,0.008,,,,,
RF,,0.105,0.006,,,,,
RNN,,,,,I saw that the tuning results didn't finish and I forgot I was running this test,,,
RNN,"Tuning tests, trying optimizers and 50 time",,,,Heh actually this didn't finish cause it was overnight.,,,
NN,,,,,,,,
LR,,,,,,,,
BLR,,,,,,,,
RF,,,,,,,,
NN,Using light exponential decay,0.098,0.008,0.103,Whoa that's actually pretty amazing. 32.28% deviation from predict-mean on test. Pretty much same results as previous NN submissions,0.62,output_f32.64_features.nn.04_18.csv,
RNN,The predictions are pretty crazy,0.094,0.008,0.094,best yet by far. predictions deviate by 223.02%,-0.02,output_f32.64_features.rnn.04_18.csv,
NN,Tuning input dropout and retuning activation,0.095,0.009,,Note: This is without learning rate schedule cause I don't have a good way to make it work with random search,,,
NN,Rerunning,0.095,0.008,,,,,
RNN,Tuning results,0.095,0.007,,The lower stddev might be from input dropout. It didn't actually test many learning rates though.,,,
RNN ,Tuning results,0.093,0.007,,"Posttrain no correlation, learning rate dominant effect, input dropout need more data. Also note, no model actually reaches 500 epochs - most end around 150",,,
RNN,No tuning but trying time=10,0.094,0.008,,It's a very slight improvement I think but takes forever to run.,,,
RNN,Everything constant but trying postfit,0.095,0.008,,,,,
RNN,Tuning results,0.093,0.009,,Best model has a little higher input dropout although that wasn't correlated overall. More hidden units was worse,,,
RNN,No tuning,0.093,0.008,,,,,
RNN,Same but time=8,0.093,0.008,,,,,
RNN,"Frozen settings, added some dmop",0.094,0.008,0.091,I'm not sure why it's worse in training; tests showed that the new features helped. This test score is amazing though.,-0.40,,
"Hourly, old features, trying out TensorFlow",,,,,,,,
LR,,0.103,0.009,,,,,
EN,,0.101,0.008,,,,,
NN,,0.096,0.008,,This is basically the same as Theano,,,
RNN,time=4,0.094,0.008,,It's slightly better but not too much different.,,,
"Hourly, old features, back to Theano but tweaking batch size based on GPU findings - 4x batches were helpful",,,,,,,,
LR,,0.103,0.009,,,,,
EN,,0.101,0.008,,,,,
NN,,0.096,0.008,,"15.7 min, down from around 30 I think",,,
RNN,,0.095,0.008,,"35 min, but all the runs stopped prematurely",,,
RNN,Trying LR 2e-3 instead of 4,0.094,0.008,,I think this worked out much better but it took 55 min. Still a small improvement I think,,,
RNN,1000 epochs,0.094,0.007,,Hmmm I thought I got like 0.931 last time I did a full train,,,
Back to new features,,,,,,,,
LR,,0.101,0.009,,New features better in LR,,,
EN,,0.100,0.008,,New features better in EN,,,
NN,,0.096,0.008,,,,,
RNN,Early stopping again,0.094,0.008,,No real change,,,
Old features,,,,,,,,
NN,30 iter tuning,0.095,0.009,,Some interdependencies that I need to look into,,,
RNN,Tuning time,0.094,0.008,,Tough to say but it seems like more time is good,,,
"New features, checking that rolling with min periods 1 doesn't break anything",,,,,,,,
LR,,0.102,0.009,,It's really odd that it matters at all.,,,
EN,,0.101,0.008,,,,,
NN,,0.096,0.009,,This is way worse than before...,,,
RNN,,0.094,0.008,,This is the same... I don't know what to make of it,,,
"Reverting, making a mega saaf stddev one that's the sum of the 4 component stddevs",,,,,,,,
LR,,0.101,0.009,,Better than before,,,
EN,,0.100,0.008,,Slightly better than before,,,
NN,,0.096,0.009,,About the same as before,,,
RNN,,0.093,0.009,,Really good result but it could just be luck,,,
NN,second run,0.096,0.008,,,,,
RNN,second run,0.093,0.009,,Huh this feature change seems really good,,,
RNN,trying 3-step search with LR decay option,0.094,0.007,,Lower std is encouraging but we didn't find any good model (actually all worse than defaults),,,
Redoing the DMOP/EVTF counts and SAAF to match the time sample more closely,,,,,,,,
LR,,0.101,0.009,,,,,
EN,,0.100,0.008,,,,,
NN,,0.095,0.008,,,,,
RNN,,0.096,0.009,,Huh... such a small change destroyed the RNN. I wonder if I just need different LR or something - I'll try retuning.,,,
NN,second run,0.096,0.008,,"Oh interesting, that previous run may have just been a fluke",,,
Removed a the time-instant features that I didn't intend to include in the first place,,,,,,,,
LR,,0.101,0.009,,,,,
EN,,0.100,0.008,,,,,
NN,,0.096,0.009,,Seems to do 10% more rows/sec and the first CV at least had way fewer epochs than usual. Odd that it's a bit worse but I feel more comfortable with it.,,,
RNN,5 step tuning,0.094,0.009,,Training is a bit faster. The best of the tuning runs isn't that much different than the frozen settings.,,,
NN,,0.095,0.008,,,,,
RNN,0.2 rec drop,0.095,0.008,,,,,
Redid all derived features,,,,,,,,
LR,,0.107,0.004,,"Way worse, prob just overfitting",,,
EN,,0.100,0.008,,,,,
NN,,0.101,0.012,,Whoa this is amazingly bad!,,,
RNN,,0.099,0.009,,Amazingly bad,,,
Trying RobustScaler,,,,,,,,
LR,,0.107,,,,,,
EN,,0.100,,,,,,
NN,,0.103,,,,,,
RNN,,,,,,,,
Note: I've mostly switched to logging experimental results in the Trello comments cause it's easier to copy/paste and it loads faster,,,,,,,,
Syncing to leaderboard,,,,,,,,
LR,,0.102,0.010,,,,,
EN,,0.099,0.008,,,,,
NN,"clip to value, augment mean, otherwise normal",0.093,0.007,0.096,"CV results from experiment script not regular train test. This is actually the best MLP on held-out testing data but it's clear that the MLP is overfitting. The z-score is a bit closer than the last MLP I tried for what it's worth. 44% change from base, 80% in train range",0.46,,
RNN,plain,0.093,0.007,,,,,
RNN,"clip to value, augment mean, ensemble ot 2",0.092,0.007,0.094,"CV results are a guess. 21% change from base, 93% in train range",0.20,,
RNN,"clip to value, augment mean, ensemble ot 2, time 8",0.093,0.007,0.092,"20.24% change from base, 96% in train range. Daily trends actually a bit diff than RNN T4",-0.02,,
RNN,same time=12,0.092,0.007,,,,,
Mean of previous two,,,,0.092,VERY close to the better of the two models which is impressive,,,
Mean of RNN8 + MLP w/2min saaf,,,,0.093,I guess MLP is harmful after all,,,
Experimentation on SAAF histograms and DMOP subsystem counts,,,,,,,,
RNN,"x2 T=8, all new DMOP",0.090,0.008,0.094,"The CV results are from x1, T=4 but it's usually similar",0.56,,
RNN,"x2 T=8, revert to manual DMOP list",0.092,,0.093,"Apparently I didn't save the actual output. CV results again are x1, T=4",,,
RNN,x2 T=4 manual DMOP list,0.092,,0.091,,,,
RNN,"T=4, top 20 DMOP minus the bad ones",0.091,0.007,,Seems better in many ways,,,
From this point onwards I've switched to TimeCV so the numbers aren't comparable,,,,,,,,
,,,,,,,,
RNN,"x2 T4 DMOP 18 subsys, 10 cmd",0.082,0.011,0.093,,,,
MLP,relu out,0.088,0.009,,,,,
"Dropping 10 DMOP commands, dropping some AOS, etc",,,,,,,,
MLP,,0.088,0.008,,,,,
RNN,,0.084,0.013,0.097,Submitting this despite regression so I can understand new CV correlation. Well it seems to correlate lol,,,
RNN,disabled ReLU,0.081,0.010,0.093,CV is a step in the right direction. Test is not really but I did this without x2 so I think I've been getting a lot of benefit from averaging.,,,
"Tons of tuning, found out that input noise hurt the RNN, down to 79 features from dropping some of the first-year-only ones",,,,,,,,
RNN,"x2, T4, noise 0.05",0.079,0.008,0.091,I evaluated this directly with x2 and nonneg clipping so it's a bit opimistic,,,
RNN,"x2, T4, noise 0.01",0.079,0.008,0.093,,,,
RNN,"x2, T12, noise 0.05",0.077,0.007,0.095,Graph looks less spikey but man this is a horrible result,,,